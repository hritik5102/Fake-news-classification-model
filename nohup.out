2021-04-30 17:22:18.600038: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2021-04-30 17:22:18.600200: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Device:  cpu
 * Serving Flask app "gpt-server" (lazy loading)
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
C:\Users\Heet Sakaria\Anaconda3\python.exe: can't open file 'Server-BERT/server-bert.py': [Errno 2] No such file or directory
2021-04-30 17:25:26.193929: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2021-04-30 17:25:26.194008: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
 * Serving Flask app "lstm-server" (lazy loading)
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on http://127.0.0.1:7000/ (Press CTRL+C to quit)
127.0.0.1 - - [30/Apr/2021 17:26:23] "[37mGET / HTTP/1.1[0m" 200 -
127.0.0.1 - - [30/Apr/2021 17:26:23] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2021-04-30 17:30:28.626852: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found
2021-04-30 17:30:28.661810: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)
2021-04-30 17:30:28.691417: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: Heet-Desktop
2021-04-30 17:30:28.691792: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: Heet-Desktop
2021-04-30 17:30:28.933220: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2021-04-30 17:30:31.114000: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x230cbc5c380 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-04-30 17:30:31.114101: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
WARNING:tensorflow:From C:\Users\Heet Sakaria\Desktop\Projects\Fake-news-classification-model\Server-LSTM\biLSTMInference.py:167: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.
Instructions for updating:
Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.pred127.0.0.1 - - [30/Apr/2021 17:30:51] "[37mPOST /predict HTTP/1.1[0m" 200 -
e.g. if it uses a `sigmoid` last-layer activation).
127.0.0.1 - - [30/Apr/2021 17:30:47] "[37mPOST /predict HTTP/1.1[0m" 200 -
127.0.0.1 - - [30/Apr/2021 17:37:51] "[37mGET / HTTP/1.1[0m" 200 -
127.0.0.1 - - [30/Apr/2021 17:37:51] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
127.0.0.1 - - [30/Apr/2021 17:37:52] "[37mGET / HTTP/1.1[0m" 200 -
127.0.0.1 - - [30/Apr/2021 17:37:56] "[37mGET / HTTP/1.1[0m" 200 -
127.0.0.1 - - [30/Apr/2021 17:39:06] "[37mGET / HTTP/1.1[0m" 200 -
2021-04-30 17:41:21.988550: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2021-04-30 17:41:21.988813: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Device:  cpu
 * Serving Flask app "gpt-server" (lazy loading)
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
127.0.0.1 - - [30/Apr/2021 17:41:48] "[37mGET / HTTP/1.1[0m" 200 -
127.0.0.1 - - [30/Apr/2021 17:49:41] "[37mGET / HTTP/1.1[0m" 200 -
127.0.0.1 - - [30/Apr/2021 17:51:54] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
127.0.0.1 - - [30/Apr/2021 18:18:56] "[1m[31mPOST / HTTP/1.1[0m" 405 -
127.0.0.1 - - [30/Apr/2021 18:19:42] "[1m[31mPOST / HTTP/1.1[0m" 405 -
127.0.0.1 - - [30/Apr/2021 18:21:02] "[1m[31mPOST / HTTP/1.1[0m" 405 -
127.0.0.1 - - [30/Apr/2021 18:23:44] "[37mPOST /predict HTTP/1.1[0m" 200 -
127.0.0.1 - - [30/Apr/2021 18:25:39] "[37mPOST /predict HTTP/1.1[0m" 200 -
127.0.0.1 - - [30/Apr/2021 18:26:48] "[37mPOST /predict HTTP/1.1[0m" 200 -
Token indices sequence length is longer than the specified maximum sequence length for this model (1860 > 1024). Running this sequence through the model will result in indexing errors
[2021-04-30 18:38:09,513] ERROR in app: Exception on /predict [POST]
Traceback (most recent call last):
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\flask\app.py", line 2292, in wsgi_app
    response = self.full_dispatch_request()
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\flask\app.py", line 1815, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\flask\app.py", line 1718, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\flask\_compat.py", line 35, in reraise
    raise value
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\flask\app.py", line 1813, in full_dispatch_request
    rv = self.dispatch_request()
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\flask\app.py", line 1799, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "Server-GPT2/gpt-server.py", line 14, in predict
    label = gptModel.predict(text)
  File "C:\Users\Heet Sakaria\Desktop\Projects\Fake-news-classification-model\Server-GPT2\GPT2Inference.py", line 28, in predict
    outputs = self.gpt_model(**inputs)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\transformers\models\gpt2\modeling_gpt2.py", line 1240, in forward
    return_dict=return_dict,
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\transformers\models\gpt2\modeling_gpt2.py", line 695, in forward
    position_embeds = self.wpe(position_ids)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\torch\nn\modules\sparse.py", line 158, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\torch\nn\functional.py", line 1916, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
127.0.0.1 - - [30/Apr/2021 18:38:10] "[1m[35mPOST /predict HTTP/1.1[0m" 500 -
 Host, Default Version
127.0.0.1 - - [30/Apr/2021 18:25:22] "[37mPOST /predict H127.0.0.1 - - [30/Apr/2021 18:26:20] "[37mPOST /predict HTTP/1.1[0m" 200 -
127.0.0.1 - - [30/Apr/2021 18:27:26] "[37mPOST /predict HTTP/1.1[0m" 200 -
127.0.0.1 - - [30/Apr/2021 18:39:06] "[37mPOST /predict HTTP/1.1[0m" 200 -
ation-model\Server-BERT\BertInference.py:111: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  pred_token_ids = np.array(list(pred_token_ids))
[2021-04-30 18:38:58,863] ERROR in app: Exception on /predict [POST]
Traceback (most recent call last):
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\flask\app.py", line 2292, in wsgi_app
    response = self.full_dispatch_request()
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\flask\app.py", line 1815, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\flask\app.py", line 1718, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\flask\_compat.py", line 35, in reraise
    raise value
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\flask\app.py", line 1813, in full_dispatch_request
    rv = self.dispatch_request()
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\flask\app.py", line 1799, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File "Server-BERT/bert-server.py", line 14, in predict
    label, loss  = bertModel.predict(text)
  File "C:\Users\Heet Sakaria\Desktop\Projects\Fake-news-classification-model\Server-BERT\BertInference.py", line 118, in predict
    predictions = model.predict(pred_token_ids)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py", line 88, in _method_wrapper
    return method(self, *args, **kwargs)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py", line 1249, in predict
    model=self)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py", line 1112, in __init__
    model=model)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py", line 265, in __init__
    x, y, sample_weights = _process_tensorlike((x, y, sample_weights))
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py", line 1013, in _process_tensorlike
    inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\tensorflow\python\util\nest.py", line 617, in map_structure
    structure[0], [func(*x) for x in entries],
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\tensorflow\python\util\nest.py", line 617, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py", line 1008, in _convert_numpy_and_scipy
    return ops.convert_to_tensor(x, dtype=dtype)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py", line 1341, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\tensorflow\python\framework\tensor_conversion_registry.py", line 52, in _default_conversion_function
    return constant_op.constant(value, dtype, name=name)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py", line 262, in constant
    allow_broadcast=True)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py", line 270, in _constant_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File "C:\Users\Heet Sakaria\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py", line 96, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).
127.0.0.1 - - [30/Apr/2021 18:38:59] "[1m[35mPOST /predict HTTP/1.1[0m" 500 -
